# Making Predictions with the Human Activity Recognition Dataset

_David Dobor_ 

Mini Project for “Practical Machine Learning”" from Johns Hopkins University.

_Background and Data sections are copied (but slightly modified) from the course’s assignement page_.


## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is _quantify how much_ of a particular activity they do, but they _rarely quantify how well_ they do it. 

This project uses data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The goal of this project is to predict the manner in which they did the exercise. This is the `classe` variable in the training set. Any other variable may be used to predict `classe`. This report describes how the model is built, how cross validation is used, reports the expected out of sample error, and explains why the choices that were made were made.

### Data

The training data for this project are available here:

+ https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

+ https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


After downloading the data from these links, we load them into `R`:

```{r warning=FALSE, message=FALSE}
train <- read.csv("./pml-training.csv", head=TRUE, sep=",", na.strings=c("NA","#DIV/0!","")) 
test <- read.csv("./pml-testing.csv", head=TRUE, sep=",", na.strings=c("NA","#DIV/0!",""))
```


The dimensions of the data are as follows:

+ training data : 19622 rows,  160 columns
+ testing data  :    20 rows,  160 columns

`classe` variable (variable to precict) is categorical with levels: 
```{r, echo=FALSE}
table(train$classe)
```


_Note: To reproduce the following results, the random seed must be set to `1234`._

## Preprocessing 

The data contain missing values, as well as features that will not be useful in predicion. We preprocess the data as indicated in the comments of the following code:

```{r warning=FALSE, message=FALSE}
set.seed(1234)
# Get rid of columns consisting of missing values only
train<-train[,colSums(is.na(train)) == 0]
test <-test[,colSums(is.na(test)) == 0]

# These variables are irrelevant to the analysis that follows:
# user_name, raw_timestamp_part_1, raw_timestamp_part_,2 cvtd_timestamp, new_window, and  num_window (columns 1 to 7). 
# We therefore delete these variables:
train   <-train[,-c(1:7)]
test <-test[,-c(1:7)]

# we are left with following datasets:
dim(train)
dim(test)

#head(train)
#head(test)
```

We will need the following libraries in what follows:
```{r warning=FALSE, message=FALSE}
library(caret)
library(rpart) # Regressive Partitioning and Regression trees
library(rpart.plot) # Decision Tree plot
library(randomForest)
```

## Partitioning the Training Set
We now partition the traing data into two subsets to perform cross-validation. 
The split will be 75% for training / 25% for validation.

```{r warning=FALSE, message=FALSE, cache=TRUE}
trainIndeces <- createDataPartition(y=train$classe, p=0.75, list=FALSE)
subTraining <- train[trainIndeces, ] 
subTesting <- train[-trainIndeces, ]

dim(subTraining)
dim(subTesting)

#head(subTraining)
#head(subTesting)
```

## Taking a Quick Look at the Label Distribution
We plot the frequency of the labels. The 5 levels of `classe` variables in `subTraining` are distributed as follows:

```{r warning=FALSE, message=FALSE, cache=TRUE}
plot(subTraining$classe, col="lightsteelblue3", main="Class Label Distribution in the subTraining Data", xlab="Class Labels", ylab="Frequency")
#head(subTesting)
```

# Predictions
We try two different methods for predicting the class labels: Decision Trees and Random Forests. In the end we will select the model that attains best accuracy. 

## Predicting Using Decision Trees 

```{r warning=FALSE, message=FALSE, cache=TRUE}
mod.DecisionTree <- rpart(classe ~ ., data=subTraining, method="class")
prediction1 <- predict(mod.DecisionTree, subTesting, type = "class")
rpart.plot(mod.DecisionTree, main="Classification Tree", extra=100, under=TRUE, faclen=0)

# Let's see how good the prediction is:
confusionMatrix(prediction1, subTesting$classe)
```

We see that the accuracy is actually horrible here, so we would like to try some other method.
(We could try to prune or otherwise fine-tune the tree for better results, but we try another method first.)



## A Better Model: Random Forest 

We note that we have a large number of features that can be potentially used for prediction. One advantage of using random forests for classificaton is that the algorithm automatically selects the best features for prediction. It is also robust to outliers. 

We use a random forest with 10 folds for cross validation and limit the number of trees to 200. 

```{r warning=FALSE, message=FALSE, cache=TRUE}

ctrl <- trainControl(method = "cv", number=10)
mod.randForest  <- train(classe ~ .,
                        data = subTraining,
                        method = "rf",  
                        trControl = ctrl,
                        allowParallel=TRUE,
                        ntree=200)

print(mod.randForest)
```


## Some Plots of the Fitted Model

Plot that demonstrates the order of importance of the features:

```{r warning=FALSE, message=FALSE, cache=TRUE}
print(plot(varImp(mod.randForest)))
```

The confusion matrix is as follows:

```{r warning=FALSE, message=FALSE, cache=TRUE}
#mod.randForest <- randomForest(classe ~. , data=subTraining, method="class")
prediction2 <- predict(mod.randForest, subTesting, type = "raw")
confusionMatrix(prediction2, subTesting$classe)
```

So the Random Forest has an excellent accuracy. 


## Final Prediction        

```{r warning=FALSE, message=FALSE, cache=TRUE}
# Predict outcome labels on the original Testing data set
predictfinal <- predict(mod.randForest, test, type="raw")
predictfinal
```{r warning=FALSE, message=FALSE, cache=TRUE}





